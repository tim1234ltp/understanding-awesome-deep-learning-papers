Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
[ ] Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury [The shared views of four research groups]

© ISTOCKPHOTO.COM/SUCHOA LERTADIPAT

FUNDAMENTAL TECHNOLOGIES IN MODERN SPEECH RECOGNITION

Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior proba-
Digital Object Identifier 10.1109/MSP.2012.2205597 Date of publication: 15 October 2012

bilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.
INTRODUCTION New machine learning algorithms can lead to significant advances in automatic speech recognition (ASR). The biggest

IEEE SIGNAL PROCESSING MAGAZINE [82] NOVEMBER 2012

1053-5888/12/$31.00©2012IEEE

single advance occurred nearly four decades ago with the introduction of the expectation-maximization (EM) algorithm for training HMMs (see [1] and [2] for informative historical reviews of the introduction of HMMs).

DEEP NEURAL NETWORKS THAT HAVE MANY HIDDEN LAYERS AND ARE
TRAINED USING NEW METHODS HAVE BEEN SHOWN TO OUTPERFORM GMMs ON A VARIETY OF SPEECH RECOGNITION BENCHMARKS,

acoustic modeling if they can more effectively exploit information embedded in a large window of frames.
Artificial neural networks trained by backpropagating error derivatives have the poten-

With the EM algorithm, it be-

SOMETIMES BY A LARGE MARGIN.

tial to learn much better models

came possible to develop speech

of data that lie on or near a non-

recognition systems for real-

linear manifold. In fact, two

world tasks using the richness of GMMs [3] to represent the decades ago, researchers achieved some success using artificial

relationship between HMM states and the acoustic input. In neural networks with a single layer of nonlinear hidden units

these systems the acoustic input is typically represented by con- to predict HMM states from windows of acoustic coefficients

catenating Mel-frequency cepstral coefficients (MFCCs) or per- [9]. At that time, however, neither the hardware nor the learn-

ceptual linear predictive coefficients (PLPs) [4] computed from ing algorithms were adequate for training neural networks

the raw waveform and their first- and second-order temporal with many hidden layers on large amounts of data, and the

differences [5]. This nonadaptive but highly engineered prepro- performance benefits of using neural networks with a single

cessing of the waveform is designed to discard the large amount hidden layer were not sufficiently large to seriously challenge

of information in waveforms that is considered to be irrelevant GMMs. As a result, the main practical contribution of neural

for discrimination and to express the remaining information in networks at that time was to provide extra features in tandem

a form that facilitates discrimination with GMM-HMMs.

or bottleneck systems.

GMMs have a number of advantages that make them suit-

Over the last few years, advances in both machine learning

able for modeling the probability distributions over vectors of algorithms and computer hardware have led to more efficient

input features that are associated with each state of an HMM. methods for training DNNs that contain many layers of non-

With enough components, they can model probability distri- linear hidden units and a very large output layer. The large

butions to any required level of accuracy, and they are fairly output layer is required to accommodate the large number of

easy to fit to data using the EM algorithm. A huge amount of HMM states that arise when each phone is modeled by a num-

research has gone into finding ways of constraining GMMs to ber of different "triphone" HMMs that take into account the

increase their evaluation speed and to optimize the tradeoff phones on either side. Even when many of the states of these

between their flexibility and the amount of training data triphone HMMs are tied together, there can be thousands of

required to avoid serious overfitting [6].

tied states. Using the new learning methods, several different

The recognition accuracy of a GMM-HMM system can be research groups have shown that DNNs can outperform GMMs

further improved if it is discriminatively fine-tuned after it has at acoustic modeling for speech recognition on a variety of

been generatively trained to maximize its probability of gener- data sets including large data sets with large vocabularies.

ating the observed data, especially if the discriminative objec-

This review article aims to represent the shared views of

tive function used for training is closely related to the error research groups at the University of Toronto, Microsoft Research

rate on phones, words, or sentences [7]. The accuracy can also (MSR), Google, and IBM Research, who have all had recent suc-

be improved by augmenting (or concatenating) the input fea- cesses in using DNNs for acoustic modeling. The article starts by

tures (e.g., MFCCs) with "tandem" or bottleneck features gen- describing the two-stage training procedure that is used for fit-

erated using neural networks [8], [69]. GMMs are so successful ting the DNNs. In the first stage, layers of feature detectors are

that it is difficult for any new method to outperform them for initialized, one layer at a time, by fitting a stack of generative

acoustic modeling.

models, each of which has one layer of latent variables. These

Despite all their advantages, GMMs have a serious short- generative models are trained without using any information

coming--they are statistically inefficient for modeling data about the HMM states that the acoustic model will need to dis-

that lie on or near a nonlinear manifold in the data space. For criminate. In the second stage, each generative model in the

example, modeling the set of points that lie very close to the stack is used to initialize one layer of hidden units in a DNN and

surface of a sphere only requires a few parameters using an the whole network is then discriminatively fine-tuned to predict

appropriate model class, but it requires a very large number of the target HMM states. These targets are obtained by using a

diagonal Gaussians or a fairly large number of full-covariance baseline GMM-HMM system to produce a forced alignment.

Gaussians. Speech is produced by modulating a relatively

In this article, we review exploratory experiments on the

small number of parameters of a dynamical system [10], [11], TIMIT database [12], [13] that were used to demonstrate the

and this implies that its true underlying structure is much power of this two-stage training procedure for acoustic mod-

lower-dimensional than is immediately apparent in a window eling. The DNNs that worked well on TIMIT were then applied

that contains hundreds of coefficients. We believe, therefore, to five different large-vocabulary continuous speech recogni-

that other types of model may work better than GMMs for tion (LVCSR) tasks by three different research groups whose

IEEE SIGNAL PROCESSING MAGAZINE [83] NOVEMBER 2012

results we also summarize. The DNNs worked well on all of The update rule for biases can be derived by treating them as

these tasks when compared with highly tuned GMM-HMM weights on connections coming from units that always have a

systems, and on some of the tasks they outperformed the state state of one.

of the art by a large margin. We

To reduce overfitting, large

also describe some other uses of DNNs for acoustic modeling and some variations on the training procedure.
TRAINING DEEP NEURAL NETWORKS

OVER THE LAST FEW YEARS, ADVANCES IN BOTH MACHINE LEARNING ALGORITHMS AND COMPUTER HARDWARE HAVE LED TO MORE EFFICIENT METHODS
FOR TRAINING DNNs.

weights can be penalized in proportion to their squared magnitude, or the learning can simply be terminated at the point at which performance on a held-out validation set starts getting worse [9]. In DNNs with full connectivity between adja-

A DNN is a feed-forward, artificial

cent layers, the initial weights are

neural network that has more than one layer of hidden units given small random values to prevent all of the hidden units in a

between its inputs and its outputs. Each hidden unit, j, typically layer from getting exactly the same gradient.

uses the logistic function (the closely related hyberbolic tangent

DNNs with many hidden layers are hard to optimize.

is also often used and any function with a well-behaved deriva- Gradient descent from a random starting point near the origin

tive can be used) to map its total input from the layer below, is not the best way to find a good set of weights, and unless the

x j, to the scalar state, y j that it sends to the layer above.

initial scales of the weights are carefully chosen [15], the backpropagated gradients will have very different magnitudes in dif-

yj

=

logistic (x j)

=

1

1 +e

-x

j

,

/ x j = b j + yi wij , (1)
i

ferent layers. In addition to the optimization issues, DNNs may generalize poorly to held-out test data. DNNs with many hidden layers and many units per layer are very flexible models with a

where b j is the bias of unit j, i is an index over units in the very large number of parameters. This makes them capable of

layer below, and wij is the weight on a connection to unit j modeling very complex and highly nonlinear relationships

from unit i in the layer below. For multiclass classification, between inputs and outputs. This ability is important for high-

output unit j converts its total input, x j , into a class probabil- quality acoustic modeling, but it also allows them to model spu-

ity, p j , by using the "softmax" nonlinearity

rious regularities that are an accidental property of the

/ p j =

exp (x j) , exp (xk)

k

particular examples in the training set, which can lead to severe (2) overfitting. Weight penalties or early stopping can reduce the
overfitting but only by removing much of the modeling power.

where k is an index over all classes.

Very large training sets [16] can reduce overfitting while pre-

DNNs can be discriminatively trained (DT) by backpropa- serving modeling power, but only by making training very com-

gating derivatives of a cost function that measures the discrep- putationally expensive. What we need is a better method of

ancy between the target outputs and the actual outputs using the information in the training set to build multiple lay-

produced for each training case [14]. When using the softmax ers of nonlinear feature detectors.

output function, the natural cost function C is the cross entro-

py between the target probabilities d and the outputs of the GENERATIVE PRETRAINING

softmax, p

Instead of designing feature detectors to be good for discrimi-

C = - / d j log p j,
j

nating between classes, we can start by designing them to be (3) good at modeling the structure in the input data. The idea is to
learn one layer of feature detectors at a time with the states of

where the target probabilities, typically taking values of one or the feature detectors in one layer acting as the data for training

zero, are the supervised information provided to train the the next layer. After this generative "pretraining," the multiple

DNN classifier.

layers of feature detectors can be used as a much better start-

For large training sets, it is typically more efficient to com- ing point for a discriminative "fine-tuning" phase during which

pute the derivatives on a small, random "minibatch" of training backpropagation through the DNN slightly adjusts the weights

cases, rather than the whole training set, before updating the found in pretraining [17]. Some of the high-level features cre-

weights in proportion to the gradient. This stochastic gradient ated by the generative pretraining will be of little use for dis-

descent method can be further improved by using a "momen- crimination, but others will be far more useful than the raw

tum" coefficient, 0 1 a 11, that smooths the gradient comput- inputs. The generative pretraining finds a region of the weight-

ed for minibatch t, thereby damping oscillations across ravines space that allows the discriminative fine-tuning to make rapid

and speeding progress down ravines

progress, and it also significantly reduces overfitting [18].

A single layer of feature detectors can be learned by fitting a

Dwij (t)

=

aDwij

(t

-

1)

-

e

2C 2wij (t)

.

(4) generative model with one layer of latent variables to the input data. There are two broad classes of generative model to choose

IEEE SIGNAL PROCESSING MAGAZINE [84] NOVEMBER 2012

from. A directed model generates data by first choosing the and the probability that the network assigns to a visible vector,

states of the latent variables from a prior distribution and then v, is given by summing over all possible hidden vectors

choosing the states of the observable variables from their conditional distributions given the latent states. Examples of directed models with one layer of latent variables are factor analysis, in

/ p(v) =

1 Z

h

e-E(v, h) .

(7)

which the latent variables are drawn from an isotropic The derivative of the log probability of a training set with

Gaussian, and GMMs, in which they are drawn from a discrete respect to a weight is surprisingly simple

distribution. An undirected model has a very different way of generating data. Instead of using one set of parameters to define a prior distribution over the latent variables and a separate set

/ 1
N

n= N n=1

2 log p (vn) 2wij

=1vi h j2data - 1vi h j2model ,

(8)

of parameters to define the condition-

where N is the size of the

al distributions of the observable variables given the values of the latent variables, an undirected model uses a single set of parameters, W, to define the joint probability of a vector of values of the observable variables, v, and

WHAT WE NEED IS A BETTER METHOD OF USING THE INFORMATION
IN THE TRAINING SET TO BUILD MULTIPLE LAYERS OF NONLINEAR
FEATURE DETECTORS.

training set and the angle brackets are used to denote expectations under the distribution specified by the subscript that follows. The simple derivative in (8)

a vector of values of the latent vari-

leads to a very simple learn-

ables, h, via an energy function, E

ing rule for performing sto-

/ p(v, h; W) =

1 Z

e-E(v, h;

W),

Z=

e-E(vl, hl; W),

vl, hl

chastic steepest ascent in the log probability of the training data

(5)

Dwij = e^1vi h j2data -1vi h j2modelh,

(9)

where Z is called the partition function.

If many different latent variables interact nonlinearly to where e is a learning rate.

generate each data vector, it is difficult to infer the states of

The absence of direct connections between hidden units in

the latent variables from the observed data in a directed an RBM makes it is very easy to get an unbiased sample of

model because of a phenomenon known as "explaining away" 1vi h j2data . Given a randomly selected training case, v, the

[19]. In undirected models, however, inference is easy pro- binary state, h j , of each hidden unit, j, is set to one with prob-

vided the latent variables do not have edges linking them. ability

Such a restricted class of undirected models is ideal for layerwise pretraining because each layer will have an easy inference procedure.

/ p (h j = 1; v) = logistic (b j + vi wij)

(10)

i

We start by describing an approximate learning algorithm and vi h j is then an unbiased sample. The absence of direct con-

for a restricted Boltzmann machine (RBM) which consists of a nections between visible units in an RBM makes it very easy to

layer of stochastic binary "visible" units that represent binary get an unbiased sample of the state of a visible unit, given a hid-

input data connected to a layer of stochastic binary hidden units den vector

that learn to model significant nonindependencies between the visible units [20]. There are undirected connections between visible and hidden units but no visible-visible or hidden-hidden

/ p (vi = 1; h) = logistic (ai + h j wij) .

(11)

j

connections. An RBM is a type of Markov random field (MRF)

Getting an unbiased sample of 1vi h j2model , however, is

but differs from most MRFs in several ways: it has a bipartite much more difficult. It can be done by starting at any random

connectivity graph, it does not usually share weights between state of the visible units and performing alternating Gibbs sam-

different units, and a subset of the variables are unobserved, pling for a very long time. Alternating Gibbs sampling consists

even during training.

of updating all of the hidden units in parallel using (10) fol-

lowed by updating all of the visible units in parallel using (11).

AN EFFICIENT LEARNING PROCEDURE FOR RBMs

A much faster learning procedure called contrastive diver-

A joint configuration, (v, h) of the visible and hidden units of an gence (CD) was proposed in [20]. This starts by setting the states

RBM has an energy given by

of the visible units to a training vector. Then the binary states of

/ / / E(v, h) = -

aivi -

b j h j - vi h j wij , (6)

i ! visible

j ! hidden

i, j

the hidden units are all computed in parallel using (10). Once binary states have been chosen for the hidden units, a "reconstruction" is produced by setting each vi to one with a probabil-

where vi, h j are the binary states of visible unit i and hidden ity given by (11). Finally, the states of the hidden units are

unit j, ai, b j are their biases, and wij is the weight between updated again. The change in a weight is then given by

them. The network assigns a probability to every possible pair of

a visible and a hidden vector via this energy function as in (5)

Dwij = e (1vi h j2data - 1vi h j2recon).

(12)

IEEE SIGNAL PROCESSING MAGAZINE [85] NOVEMBER 2012

A simplified version of the same learning rule that uses the that represent progressively more complex statistical structure

states of individual units instead of pairwise products is used for in the data. The RBMs in a stack can be combined in a surpris-

the biases.

ing way to produce [22] a single, multilayer generative model

CD works well even though it is only crudely approximating called a deep belief net (DBN) (not to be confused with a

the gradient of the log probability

dynamic Bayesian net, which

of the training data [20]. RBMs learn better generative models if more steps of alternating Gibbs sampling are used before collecting the statistics for the second term in the learning rule, but for the purposes of pretraining feature detectors, more alternations are

ONE VERY NICE PROPERTY OF A DBN THAT DISTINGUISHES IT FROM
OTHER MULTILAYER, DIRECTED, NONLINEAR GENERATIVE MODELS IS THAT IT IS POSSIBLE TO INFER THE STATES OF THE LAYERS OF HIDDEN UNITS IN A SINGLE FORWARD PASS.

is a type of directed model of temporal data that unfortunately has the same acronym). Even though each RBM is an undirected model, the DBN formed by the whole stack is a hybrid generative model whose top two layers are undi-

generally of little value and all the

rected (they are the final RBM

results reviewed here were obtained using CD1 which does a single full step of alternating Gibbs sampling after the initial

in the stack) but whose lower layers have top-down, directed connections (see Figure 1).

update of the hidden units. To suppress noise in the learning,

To understand how RBMs are composed into a DBN, it is

the real-valued probabilities rather than binary samples are gen- helpful to rewrite (7) and to make explicit the dependence on W:

erally used for the reconstructions and the subsequent states of the hidden units, but it is important to use sampled binary val-

p (v; W) = / p (h; W) p (v;h; W),

(16)

ues for the first computation of the hidden states because the

h

sampling noise acts as a very effective regularizer that prevents where p (h; W) is defined as in (7) but with the roles of the visi-

overfitting [21].

ble and hidden units reversed. Now it is clear that the model can

be improved by holding p (v;h; W) fixed after training the RBM,

MODELING REAL-VALUED DATA

but replacing the prior over hidden vectors p (h; W) by a better

Real-valued data, such as MFCCs, are more naturally modeled prior, i.e., a prior that is closer to the aggregated posterior over

by linear variables with Gaussian noise and the RBM energy hidden vectors that can be sampled by first picking a training

function can be modified to accommodate such variables, giving case and then inferring a hidden vector using (14). This aggre-

a Gaussian­Bernoulli RBM (GRBM)

gated posterior is exactly what the next RBM in the stack is

/ / / E(v,h)

=

i ! vis

(vi - ai) 2

2v

2 i

-

bjhj -
j ! hid

i, j

vi vi

h

j

wij ,

(13)

trained to model. As shown in [22], there is a series of variational bounds on
the log probability of the training data, and furthermore, each

where vi is the standard deviation of the Gaussian noise for vis- time a new RBM is added to the stack, the variational bound on

ible unit i.

the new and deeper DBN is better than the previous variational

The two conditional distributions required for CD1 learning are

bound, provided the new RBM is initialized and learned in the right way. While the existence of a bound that keeps improving

/ p (h j ; v) = logisticcb j +
i

vi vi

wij

m

/ p (vi ;h) = N cai + vi

h

j

wij,

v

2 i

m

,

j

is mathematically reassuring, it does not answer the practical (14) issue, addressed in this article, of whether the learned feature
detectors are useful for discrimination on a task that is (15) unknown while training the DBN. Nor does it guarantee that
anything improves when we use efficient short-cuts such as

where N (n, v2) is a Gaussian. Learning the standard devia- CD1 training of the RBMs.

tions of a GRBM is problematic for reasons described in [21], so

One very nice property of a DBN that distinguishes it from

for pretraining using CD1, the data are normalized so that each coefficient has zero mean and unit variance, the standard devia-

other multilayer, directed, nonlinear generative models is that it is possible to infer the states of the layers of hidden units in a

tions are set to one when computing p (v; h), and no noise is single forward pass. This inference, which is used in deriving

added to the reconstructions. This avoids the issue of deciding the variational bound, is not exactly correct but is fairly accu-

the right noise level.

rate. So after learning a DBN by training a stack of RBMs, we

can jettison the whole probabilistic framework and simply use

STACKING RBMs TO MAKE A DEEP BELIEF NETWORK

the generative weights in the reverse direction as a way of ini-

After training an RBM on the data, the inferred states of the hid- tializing all the feature detecting layers of a deterministic feed-

den units can be used as data for training another RBM that forward DNN. We then just add a final softmax layer and train

learns to model the significant dependencies between the hid- the whole DNN discriminatively. Unfortunately, a DNN that is

den units of the first RBM. This can be repeated as many times pretrained generatively as a DBN is often still called a DBN in

as desired to produce many layers of nonlinear feature detectors the literature. For clarity, we call it a DBN-DNN.

IEEE SIGNAL PROCESSING MAGAZINE [86] NOVEMBER 2012

RBM

RBM

Copy

W3

GRBM

W2

Copy

W1

DBN W3 W2 W1

DBN-DNN W4 = 0 W3T W2T W1T

[FIG1] The sequence of operations used to create a DBN with three hidden layers and to convert it to a pretrained DBN-DNN. First, a GRBM is trained to model a window of frames of real-valued acoustic coefficients. Then the states of the binary hidden units of the GRBM are used as data for training an RBM. This is repeated to create as many hidden layers as desired. Then the stack of RBMs is converted to a single generative model, a DBN, by replacing the undirected connections of the lower level RBMs by top-down, directed connections. Finally, a pretrained DBN-DNN is created by adding a "softmax" output layer that contains one unit for each possible state of each HMM. The DBN-DNN is then discriminatively trained to predict the HMM state corresponding to the central frame of the input window in a forced alignment.

INTERFACING A DNN WITH AN HMM After it has been discriminatively fine-tuned, a DNN outputs probabilities of the form p (HMMstate; AcousticInput). But to compute a Viterbi alignment or to run the forward-backward algorithm within the HMM framework, we require the likelihood p (AcousticInput ;HMMstate). The posterior probabilities that the DNN outputs can be converted into the scaled likelihood by dividing them by the frequencies of the HMM states in the forced alignment that is used for fine-tuning the DNN [9]. All of the likelihoods produced in this way are scaled by the same unknown factor of p (AcousticInput), but this has no effect on the alignment. Although this conversion appears to have little effect on some recognition tasks, it can be important for tasks where training labels are highly unbalanced (e.g., with many frames of silences).
PHONETIC CLASSIFICATION AND RECOGNITION ON TIMIT The TIMIT data set provides a simple and convenient way of testing new approaches to speech recognition. The training set is small enough to make it feasible to try many variations of a new method and many existing techniques have already been benchmarked on the core test set, so it is easy to see if a new approach is promising by comparing it with existing techniques that have been implemented by their proponents [23]. Experience has shown that performance improvements on TIMIT do not necessarily translate into performance improvements on large vocabulary tasks with less controlled recording conditions and much more training data. Nevertheless, TIMIT provides a good start-

ing point for developing a new approach, especially one that requires a challenging amount of computation.
Mohamed et. al. [12] showed that a DBN-DNN acoustic model outperformed the best published recognition results on TIMIT at about the same time as Sainath et. al. [23] achieved a similar improvement on TIMIT by applying state-of-the-art techniques developed for large vocabulary recognition. Subsequent work combined the two approaches by using stateof-the-art, DT speaker-dependent features as input to the DBNDNN [24], but this produced little further improvement, probably because the hidden layers of the DBN-DNN were already doing quite a good job of progressively eliminating speaker differences [25].
The DBN-DNNs that worked best on the TIMIT data formed the starting point for subsequent experiments on much more challenging large vocabulary tasks that were too computationally intensive to allow extensive exploration of variations in the architecture of the neural network, the representation of the acoustic input, or the training procedure.
For simplicity, all hidden layers always had the same size, but even with this constraint it was impossible to train all possible combinations of number of hidden layers [1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1,024, 2,048, 3,072], and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]. Fortunately, the performance of the networks on the TIMIT core test set was fairly insensitive to the precise details of the architecture and the results in [13] suggest that any combination of the numbers in boldface probably has an error rate within about 2% of the very best combination. This

IEEE SIGNAL PROCESSING MAGAZINE [87] NOVEMBER 2012

robustness is crucial for methods such as DBN-DNNs that have a lot of tuneable metaparameters. Our consistent finding is that multiple hidden layers always worked better than one hidden layer and, with multiple hidden layers, pretraining always improved the results on both the development and test sets in the TIMIT task. Details of the learning rates, stopping criteria, momentum, L2 weight penalties and minibatch size for both the pretraining and fine-tuning are given in [13].
Table 1 compares DBN-DNNs with a variety of other methods on the TIMIT core test set. For each type of DBN-DNN the architecture that performed best on the development set is reported. All methods use MFCCs as inputs except for the three marked "fbank" that use log Mel-scale filter-bank outputs.
PREPROCESSING THE WAVEFORM FOR DEEP NEURAL NETWORKS State-of-the-art ASR systems do not use filter-bank coefficients as the input representation because they are strongly correlated so modeling them well requires either full covariance Gaussians or a huge number of diagonal Gaussians. MFCCs offer a more suitable alternative as their individual components are roughly independent so they are much easier to model using a mixture of diagonal covariance Gaussians. DBN-DNNs do not require uncorrelated data and, on the TIMIT database, the work reported in [13] showed that the best performing DBN-DNNs trained with filter-bank features had a phone error rate 1.7% lower than the best performing DBN-DNNs trained with MFCCs (see Table 1).
FINE-TUNING DBN-DNNs TO OPTIMIZE MUTUAL INFORMATION In the experiments using TIMIT discussed above, the DNNs were fine-tuned to optimize the per frame cross entropy between the target HMM state and the predictions. The transition parameters and language model scores were obtained from an HMM-like approach and were trained independently of the

[TABLE 1] COMPARISONS AMONG THE REPORTED SPEAKER-INDEPENDENT (SI) PHONETIC RECOGNITION ACCURACY RESULTS ON TIMIT CORE TEST SET WITH 192 SENTENCES.

METHOD

PER

CD-HMM [26]

27.3%

AUGMENTED CONDITIONAL RANDOM FIELDS [26]

26.6%

RANDOMLY INITIALIZED RECURRENT NEURAL NETS [27]

26.1%

BAYESIAN TRIPHONE GMM-HMM [28]

25.6%

MONOPHONE HTMS [29]

24.8%

HETEROGENEOUS CLASSIFIERS [30]

24.4%

MONOPHONE RANDOMLY INITIALIZED DNNs (SIX LAYERS) [13]

23.4%

MONOPHONE DBN-DNNs (SIX LAYERS) [13]

22.4%

MONOPHONE DBN-DNNs WITH MMI TRAINING [31]

22.1%

TRIPHONE GMM-HMMs DT W/ BMMI [32]

21.7%

MONOPHONE DBN-DNNs ON FBANK (EIGHT LAYERS) [13]

20.7%

MONOPHONE MCRBM-DBN-DNNs ON FBANK (FIVE LAYERS) [33] 20.5%

MONOPHONE CONVOLUTIONAL DNNs ON FBANK (THREE LAYERS)

[34]

20.0%

DNN weights. However, it has long been known that sequence classification criteria, which are more directly correlated with the overall word or phone error rate, can be very helpful in improving recognition accuracy [7], [35] and the benefit of using such sequence classification criteria with shallow neural networks has already been shown by [36]­[38]. In the more recent work reported in [31], one popular type of sequence classification criterion, maximum mutual information (MMI), proposed as early as 1986 [7], was successfully applied to learn DBN-DNN weights for the TIMIT phone recognition task. MMI optimizes the conditional probability p (l1:T ;v1:T) of the whole sequence of labels, l1:T , with length T, given the whole visible feature utterance v1:T , or equivalently the hidden feature sequence h1:T extracted by the DNN

p (l1:T ; v1:T) = p (l1:T ; h1:T)

/ / / exp`
=

T t=

1

cij

zij

(lt

-

1,

lt)

+

Z (h1:T)

T t=1

D d =1

m l t, d

htd

j

,

(17)

where the transition feature zij (lt-1, lt) takes on a value of one if lt-1 = i and lt = j , and otherwise takes on a value of zero, where cij is the parameter associated with this transition feature, htd is the dth dimension of the hidden unit value at the tth frame at the final layer of the DNN, and where D is the number of units in the final hidden layer. Note the objective function of (17) derived from mutual information [35] is the same as the conditional likelihood associated with a specialized linear-chain conditional random field. Here, it is the topmost layer of the DNN below the softmax layer, not the raw speech coefficients of MFCC or PLP, that provides "features" to the conditional random field.
To optimize the log conditional probability p (l1n:T ; v1n:T) of the nth utterance, we take the gradient over the activation parameters mkd , transition parameters cij , and the lower-layer weights of the DNN, wij , according to

/ 2

log

p

(l

n 1:T

;

v1n:T)

2m kd

=

T

^d

(l

n t

=

t=1

k) -

p (ltn

=

k ; v1n:T)hhtnd

(18)

/ 2

log

p

(l

n 1:T

;

v1n:T)

2cij

T

=

6d^l

n t-

1

t=1

=

i, ltn

=

jh

-

p^ltn- 1

=

i,

l

n t

=

j ; v 1n:T h@

(19)

/ / 2

log

p

(l

n 1:T

;

v

1n:T)

2wij

=

T

K

=mltd -

t=1

k=1

p

(l

n t

=

k;v1n:T) mkdG

#

htnd

(1

-

htnd)

x

n ti

.

(20)

Note that the gradient ^2 log p (l1n:T ; v1n:T)h/ (2wij) above can be

viewed

as

back-propagating

the

error

d

(l

n t

=

k)

-

p

(l

n t

=

k ; v 1n:T),

versus d (ltn = k) - p (ltn = k;vtn) in the frame-based training

algorithm.

In implementing the above learning algorithm for a DBN-

DNN, the DNN weights can first be fine-tuned to optimize the

per frame cross entropy. The transition parameters can be ini-

tialized from the combination of the HMM transition matrices

IEEE SIGNAL PROCESSING MAGAZINE [88] NOVEMBER 2012

and the "phone language" model scores, and can be further which serves as the building block for pretraining, is an instance

optimized by tuning the transition features while fixing the of "product of experts" [20], in contrast to mixture models that

DNN weights before the joint optimization. Using the joint opti- are a "sum of experts." Product models have only very recently

mization with careful scheduling, we observe that the sequential been explored in speech processing, e.g., [41]. Mixture models

MMI training can outperform the frame-level training by about with a large number of components use their parameters ineffi-

5% relative within the same system in the same laboratory.

ciently because each parameter only applies to a very small frac-

tion of the data whereas each parameter of a product model is

CONVOLUTIONAL DNNs FOR

constrained by a large fraction of the data. Second, while both

PHONE CLASSIFICATION AND RECOGNITION

DNNs and GMMs are nonlinear models, the nature of the nonlin-

All the previously cited work reported phone recognition results earity is very different. A DNN has no problem modeling multiple

on the TIMIT database. In recognition experiments, the input is simultaneous events within one frame or window because it can

the acoustic input for the whole utterance while the output is use different subsets of its hidden units to model different events.

the spoken phonetic sequence. A

By contrast, a GMM assumes that

decoding process using a phone

each datapoint is generated by a

language model is used to pro-

THE SUCCESS OF DBN-DNNs ON TIMIT

single component of the mixture

duce this output sequence.

TASKS STARTING IN 2009 MOTIVATED

so it has no efficient way of mod-

Phonetic classification is a differ- MORE AMBITIOUS EXPERIMENTS WITH eling multiple simultaneous

ent task where the acoustic input has already been labeled with the correct boundaries between dif-

MUCH LARGER VOCABULARIES AND MORE VARIED SPEAKING STYLES.

events. Third, DNNs are good at exploiting multiple frames of input coefficients whereas GMMs

ferent phonetic units and the

that use diagonal covariance

goal is to classify these phones conditioned on the given bound- matrices benefit much less from multiple frames because they

aries. In [39], convolutional DBN-DNNs were introduced and require decorrelated inputs. Finally, DNNs are learned using sto-

successfully applied to various audio tasks including phone clas- chastic gradient descent, while GMMs are learned using the EM

sification on the TIMIT database. In this model, the RBM was algorithm or its extensions [35], which makes GMM learning

made convolutional in time by sharing weights between hidden much easier to parallelize on a cluster machine.

units that detect the same feature at different times. A max-

pooling operation was then performed, which takes the maxi- COMPARING DBN-DNNs WITH GMMs

mal activation over a pool of adjacent hidden units that share FOR LARGE-VOCABULARY SPEECH RECOGNITION

the same weights but apply them at different times. This yields The success of DBN-DNNs on TIMIT tasks starting in 2009

some temporal invariance.

motivated more ambitious experiments with much larger

Although convolutional models along the temporal dimen- vocabularies and more varied speaking styles. In this section,

sion achieved good classification results [39], applying them to we review experiments by three different speech groups on five

phone recognition is not straightforward. This is because tem- different benchmark tasks for large-vocabulary speech recogni-

poral variations in speech can be partially handled by the tion. To make DBN-DNNs work really well on large vocabulary

dynamic programing procedure in the HMM component and tasks it is important to replace the monophone HMMs used for

those aspects of temporal variation that cannot be adequately TIMIT (and also for early neural network/HMM hybrid systems)

handled by the HMM can be addressed more explicitly and effec- with triphone HMMs that have many thousands of tied states

tively by hidden trajectory models [40].

[42]. Predicting these context-dependent states provides several

The work reported in [34] applied local convolutional filters advantages over monophone targets. They supply more bits of

with max-pooling to the frequency rather than time dimension information per frame in the labels. They also make it possible

of the spectrogram. Sharing-weights and pooling over frequen- to use a more powerful triphone HMM decoder and to exploit

cy was motivated by the shifts in formant frequencies caused by the sensible classes discovered by the decision tree clustering

speaker variations. It provides some speaker invariance while that is used to tie the states of different triphone HMMs. Using

also offering noise robustness due to the band-limited nature of context-dependent HMM states, it is possible to outperform

the filters. [34] only used weight-sharing and max-pooling state-of-the-art BMMI trained GMM-HMM systems with a two-

across nearby frequencies because, unlike features that occur at hidden-layer neural network without using any pretraining

different positions in images, acoustic features occurring at very [43], though using more hidden layers and pretraining works

different frequencies are very different.

even better.

A SUMMARY OF THE DIFFERENCES BETWEEN DNNs AND GMMs Here we summarize the main differences between the DNNs and GMMs used in the TIMIT experiments described so far in this article. First, one major element of the DBN-DNN, the RBM,

BING-VOICE-SEARCH SPEECH RECOGNITION TASK The first successful use of acoustic models based on DBN-DNNs for a large vocabulary task used data collected from the Bing mobile voice search application (BMVS). The task used 24 h of training data with a high degree of acoustic variability caused by

IEEE SIGNAL PROCESSING MAGAZINE [89] NOVEMBER 2012

noise, music, side-speech, accents, sloppy pronunciation, hesita- SWITCHBOARD SPEECH RECOGNITION TASK

tion, repetition, interruptions, and mobile phone differences. The DNN-HMM training recipe developed for the Bing voice

The results reported in [42] demonstrated that the best DNN- search data was applied unaltered to the Switchboard speech rec-

HMM acoustic model trained with context-dependent states as ognition task [43] to confirm the suitability of DNN-HMM acous-

targets achieved a sentence accuracy of 69.6% on the test set, tic models for large vocabulary tasks. Before this work,

compared with 63.8% for a strong, minimum phone error DNN-HMM acoustic models had only been trained with up to

(MPE)-trained GMM-HMM baseline.

48 h of data [44] and hundreds of tied triphone states as targets,

The DBN-DNN used in the experiments was based on one of whereas this work used over 300 h of training data and thou-

the DBN-DNNs that worked well for the TIMIT task. It used five sands of tied triphone states as targets. Furthermore,

pretrained layers of hidden units with 2,048 units per layer and Switchboard is a publicly available speech-to-text transcription

was trained to classify the central frame of an 11-frame acoustic benchmark task that allows much more rigorous comparisons

context window using 761 possible context-dependent states as among techniques.

targets. In addition to demonstrating that a DBN-DNN could

The baseline GMM-HMM system on the Switchboard task

provide gains on a large vocabulary task, several other impor- was trained using the standard 309-h Switchboard-I training

tant issues were explicitly investigated in [42]. It was found that set. Thirteen-dimensional PLP features with windowed mean-

using tied triphone context-dependent state targets was crucial variance normalization were concatenated with up to third-

and clearly superior to using monophone state targets, even order derivatives and reduced to 39 dimensions by a form of

when the latter were derived from the same forced alignment linear discriminant analysis (LDA) called heteroscedastic LDA

with the same baseline. It was also confirmed that the lower the (HDLA). The SI crossword triphones used the common left-to-

error rate of the system used during forced alignment to gener- right three-state topology and shared 9,304 tied states.

ate frame-level training labels for the neural net, the lower the

The baseline GMM-HMM system had a mixture of 40

error rate of the final neural-net-based system. This effect was Gaussians per (tied) HMM state that were first trained genera-

consistent across all the alignments they tried, including mono- tively to optimize a maximum likelihood (ML) criterion and

phone alignments, alignments from ML-trained GMM-HMM then refined discriminatively to optimize a boosted maximum-

systems, and alignments from DT GMM-HMM systems.

mutual-information (BMMI) criterion. A seven-hidden-layer

Further work after that of [42] extended the DNN-HMM DBN-DNN with 2,048 units in each layer and full connectivity

acoustic model from 24 h of training data to 48 h and explored between adjacent layers replaced the GMM in the acoustic

the respective roles of pretraining and fine-tuning the DBN- model. The trigram language model, used for both systems,

DNN [44]. As expected, pretraining is helpful in training the was trained on the training transcripts of the 2,000 h of the

DBN-DNN because it initializes the DBN-DNN weights to a Fisher corpus and interpolated with a trigram model trained on

point in the weight-space from which fine-tuning is highly effec- written text.

tive. However, a moderate increase of the amount of unlabeled

The primary test set is the FSH portion of the 6.3-h Spring

pretraining data has an insignificant effect on the final recogni- 2003 National Institute of Standards and Technology rich

tion results (69.6% to 69.8%), as long as the original training transcription set (RT03S). Table 2 extracted from the litera-

set is fairly large. By contrast, the same amount of additional ture shows a summary of the core results. Using a DNN

labeled fine-tuning training data significantly improves the per- reduced the word error rate (WER) from the 27.4% of the

formance of the DNN-HMMs (accuracy from 69.6% to 71.7%).

baseline GMM-HMM (trained with BMMI) to 18.5%--a 33%

relative reduction. The DNN-HMM system

[TABLE 2] COMPARING FIVE DIFFERENT DBN-DNN ACOUSTIC MODELS WITH TWO STRONG GMM-HMM BASELINE SYSTEMS THAT ARE DISCRIMINATIVELY TRAINED. SI TRAINING ON 309 H OF DATA AND SINGLE-PASS DECODING WERE USED FOR ALL MODELS EXCEPT FOR THE GMM-HMM SYSTEM SHOWN ON THE LAST ROW WHICH USED SA TRAINING WITH 2,000 H OF DATA AND MULTIPASS DECODING INCLUDING HYPOTHESES COMBINATION. IN THE TABLE, "40 MIX" MEANS A MIXTURE OF 40 GAUSSIANS PER HMM STATE AND "15.2 NZ" MEANS 15.2 MILLION, NONZERO WEIGHTS. WERs IN % ARE SHOWN FOR TWO SEPARATE TEST SETS, HUB500-SWB AND RT03S-FSH.

trained on 309 h performs as well as combining several speaker-adaptive (SA), multipass systems that use vocal tract length normalization (VTLN) and nearly seven times as much acoustic training data (the 2,000-h Fisher corpus) (18.6%; see the last row in Table 2).
Detailed experiments [43] on the Switch-

WER

board task confirmed that the remarkable

MODELING TECHNIQUE

#PARAMS [106] HUB5'00-SWB RT03S-FSH accuracy gains from the DNN-HMM acoustic

GMM, 40 MIX DT 309H SI

29.4

23.6

27.4

model are due to the direct modeling of tied

NN 1 HIDDEN-LAYER # 4,634 UNITS

43.6

26.0

29.4

triphone states using the DBN-DNN, the effec-

+ 2 # 5 NEIGHBORING FRAMES

45.1

22.4

25.7

tive exploitation of neighboring frames by the

DBN-DNN 7 HIDDEN LAYERS # 2,048 UNITS 45.1

17.1

19.6

DBN-DNN, and the strong modeling power of

+ UPDATED STATE ALIGNMENT + SPARSIFICATION GMM 72 MIX DT 2000H SA

45.1

16.4

18.6

deeper networks, as was discovered in the Bing

15.2 NZ

16.1

18.5

voice search task [44], [42]. Pretraining the

102.4

17.1

18.6

DBN-DNN leads to the best results but it is not

IEEE SIGNAL PROCESSING MAGAZINE [90] NOVEMBER 2012

critical: For this task, it provides an absolute WER reduction of grid search to find a joint optimum of the language model

less than 1% and this gain is even smaller when using five or weight, the word insertion penalty, and the smoothing factor.

more hidden layers. For underresourced languages that have

On a test set of anonymized utterances from the live Voice

smaller amounts of labeled data, pretraining is likely to be far Input system, the DBN-DNN-based system achieved a WER of

more helpful.

12.3%--a 23% relative reduction compared to the best GMM-

Further study [45] suggests that feature-engineering tech- based system for this task. MMI sequence discriminative train-

niques such as HLDA and VTLN, commonly used in GMM- ing gave an error rate of 12.2% and model combination with the

HMMs, are more helpful for shallow neural nets than for GMM system 11.8%.

DBN-DNNs, presumably because DBN-DNNs are able to learn

appropriate features in their lower layers.

YOUTUBE SPEECH RECOGNITION TASK

In this task, the goal is to transcribe YouTube data. Unlike the

GOOGLE VOICE INPUT SPEECH RECOGNITION TASK

mobile voice input applications described above, this application

Google Voice Input transcribes voice search queries, short mes- does not have a strong language model to constrain the inter-

sages, e-mails, and user actions from mobile devices. This is a pretation of the acoustic information so good discrimination

large vocabulary task that uses a language model designed for a requires an accurate acoustic model.

mixture of search queries and

Google's full-blown baseline,

dictation.

built with a much larger training

Google's full-blown model for

PRETRAINING DNNs AS

set, was used to create approxi-

this task, which was built from a

GENERATIVE MODELS LED TO BETTER

mately 1,400 h of aligned training

very large corpus, uses an SI

RECOGNITION RESULTS ON TIMIT

data. This was used to create a

GMM-HMM model composed of context-dependent crossword triphone HMMs that have a left-to-

AND SUBSEQUENTLY ON A VARIETY OF LVCSR TASKS.

new baseline system for which the input was nine frames of MFCCs that were transformed by

right, three-state topology. This

LDA. SA training was performed,

model has a total of 7,969 senone states and uses as acoustic and decision tree clustering was used to obtain 17,552 triphone

input PLP features that have been transformed by LDA. Semi- states. STCs were used in the GMMs to model the features. The

tied covariances (STCs) are used in the GMMs to model the LDA acoustic models were further improved with BMMI. During

transformed features and BMMI [46] was used to train the decoding, ML linear regression (MLLR) and feature space MLLR

model discriminatively.

(fMLLR) transforms were applied.

Jaitly et. al. [47] used this model to obtain approximately

The acoustic data used for training the DBN-DNN acoustic

5,870 h of aligned training data for a DBN-DNN acoustic model model were the fMLLR-transformed features. The large number

that predicts the 7,969 HMM state posteriors from the acoustic of HMM states added significantly to the computational burden,

input. The DBN-DNN was loosely based on one of the DBN- since most of the computation is done at the output layer. To

DNNs used for the TIMIT task. It had four hidden layers with reduce this burden, the DNN used only four hidden layers with

2,560 fully connected units per layer and a final "softmax" layer 2,000 units in the first hidden layer and only 1,000 units in each

with 7,969 alternative states. Its input was 11 contiguous of the layers above.

frames of 40 log filter-bank outputs with no temporal deriva-

About ten epochs of training were performed on this data

tives. Each DBN-DNN layer was pretrained for one epoch as an before sequence-level training and model combination. The

RBM and then the resulting DNN was discriminatively fine- DBN-DNN gave an absolute improvement of 4.7% over the

tuned for one epoch. Weights with magnitudes below a thresh- baseline system's WER of 52.3%. Sequence-level fine-tuning of

old were then permanently set to zero before a further quarter the DBN-DNN further improved results by 0.5% and model

epoch of training. One third of the weights in the final network combination produced an additional gain of 0.9%.

were zero. In addition to the DBN-DNN training, sequence-lev-

el discriminative fine-tuning of the neural network was per- ENGLISH BROADCAST NEWS

formed using MMI, similar to the method proposed in [37]. SPEECH RECOGNITION TASK

Model combination was then used to combine results from the DNNs have also been successfully applied to an English

GMM-HMM system with the DNN-HMM hybrid, using the seg- broadcast news task. Since a GMM-HMM baseline creates the

mental conditional random field (SCARF) framework [47]. initial training labels for the DNN, it is important to have a

Viterbi decoding was done using the Google system [48] with good baseline system. All GMM-HMM systems created at IBM

modifications to compute the scaled log likelihoods from the use the following recipe to produce a state-of-the-art base-

estimates of the posterior probabilities and the state priors. line system. First, SI features are created, followed by

Unlike the other systems, it was observed that for Voice Input it SA-trained (SAT) and DT features. Specifically, given initial

was essential to smooth the estimated priors for good perfor- PLP features, a set of SI features are created using LDA.

mance. This smoothing of the priors was performed by rescal- Further processing of LDA features is performed to create

ing the log priors with a multiplier that was chosen by using a SAT features using VTLN followed by fMLLR. Finally, feature

IEEE SIGNAL PROCESSING MAGAZINE [91] NOVEMBER 2012

and model-space discriminative training is applied using the SPEEDING UP DNNs AT RECOGNITION TIME

BMMI or MPE criterion.

State pruning or Gaussian selection methods can be used to

Using alignments from a baseline system, [32] trained a make GMM-HMM systems computationally efficient at recogni-

DBN-DNN acoustic model on 50 h of data from the 1996 and tion time. A DNN, however, uses virtually all its parameters at

1997 English Broadcast News Speech Corpora [37]. The every frame to compute state likelihoods, making it potentially

DBN-DNN was trained with the

much slower than a GMM with a

best-performing LVCSR features, specifically the SAT+DT features. The DBN-DNN architecture con-

DISCRIMINATIVE PRETRAINING HAS ALSO BEEN FOUND EFFECTIVE

comparable number of parameters. Fortunately, the time that a DNN-HMM system requires to

sisted of six hidden layers with

FOR THE ARCHITECTURES CALLED

recognize 1 s of speech can be

1,024 units per layer and a final

"DEEP CONVEX NETWORK" AND

reduced from 1.6 s to 210 ms,

softmax layer of 2,220 contextdependent states. The SAT+DT feature input into the first layer used a context of nine frames. Pretraining was performed fol-

"DEEP STACKING NETWORK," WHERE PRETRAINING IS ACCOMPLISHED BY CONVEX OPTIMIZATION INVOLVING
NO GENERATIVE MODELS.

without decreasing recognition accuracy, by quantizing the weights down to 8 b and using the very fast SIMD primitives for fixed-point computation that are

lowing a recipe similar to [42].

provided by a modern x86 cen-

Two phases of fine-tuning were performed. During the first tral processing unit [49]. Alternatively, it can be reduced to

phase, the cross entropy loss was used. For cross entropy train- 66 ms by using a graphics processing unit (GPU).

ing, after each iteration through the whole training set, loss is

measured on a held-out set and the learning rate is annealed ALTERNATIVE PRETRAINING METHODS FOR DNNs

(i.e., reduced) by a factor of two if the held-out loss has grown Pretraining DNNs as generative models led to better recognition

or improves by less than a threshold of 0.01% from the previ- results on TIMIT and subsequently on a variety of LVCSR tasks.

ous iteration. Once the learning rate has been annealed five Once it was shown that DBN-DNNs could learn good acoustic

times, the first phase of fine-tuning stops. After weights are models, further research revealed that they could be trained in

learned via cross entropy, these weights are used as a starting many different ways. It is possible to learn a DNN by starting with

point for a second phase of fine-tuning using a sequence crite- a shallow neural net with a single hidden layer. Once this net has

rion [37] that utilizes the MPE objective function, a discrimi- been trained discriminatively, a second hidden layer is interposed

native objective function similar to MMI [7] but which takes between the first hidden layer and the softmax output units and

into account phoneme error rate.

the whole network is again discriminatively trained. This can be

A strong SAT+DT GMM-HMM baseline system, which con- continued until the desired number of hidden layers is reached,

sisted of 2,220 context-dependent states and 50,000 Gaussians, after which full backpropagation fine-tuning is applied.

gave a WER of 18.8% on the EARS Dev-04f set, whereas the

This type of discriminative pretraining works well in prac-

DNN-HMM system gave 17.5% [50].

tice, approaching the accuracy achieved by generative DBN pre-

training and further improvement can be achieved by stopping

SUMMARY OF THE MAIN RESULTS FOR

the discriminative pretraining after a single epoch instead of

DBN-DNN ACOUSTIC MODELS ON LVCSR TASKS

multiple epochs as reported in [45]. Discriminative pretraining

Table 3 summarizes the acoustic modeling results described has also been found effective for the architectures called "deep

above. It shows that DNN-HMMs consistently outperform convex network" [51] and "deep stacking network" [52], where

GMM-HMMs that are trained on the same amount of data, pretraining is accomplished by convex optimization involving

sometimes by a large margin. For some tasks, DNN-HMMs no generative models.

also outperform GMM-HMMs that are trained on much

Purely discriminative training of the whole DNN from ran-

more data.

dom initial weights works much better than had been thought,

provided the scales of the initial

[TABLE 3] A COMPARISON OF THE PERCENTAGE WERs USING DNN-HMMs AND GMM-HMMs ON FIVE DIFFERENT LARGE VOCABULARY TASKS.

weights are set carefully, a large amount of labeled training data is

TASK

HOURS OF

GMM-HMM

GMM-HMM

TRAINING DATA DNN-HMM WITH SAME DATA WITH MORE DATA

available, and minibatch sizes over training epochs are set appropri-

SWITCHBOARD (TEST SET 1) 309

18.5

27.4

18.6 (2,000 H)

ately [45], [53]. Nevertheless, gen-

SWITCHBOARD (TEST SET 2) 309

16.1

23.6

17.1 (2,000 H)

erative pretraining still improves

ENGLISH BROADCAST NEWS 50

17.5

18.8

test performance, sometimes by a

BING VOICE SEARCH (SENTENCE ERROR RATES)
GOOGLE VOICE INPUT
YOUTUBE

24 5,870 1,400

30.4

36.2

12.3

47.6

52.3

16.0 (22 5,870 H)

significant amount. Layer-by-layer generative pre-
training was originally done using RBMs, but various types of

IEEE SIGNAL PROCESSING MAGAZINE [92] NOVEMBER 2012

autoencoder with one hidden layer can also be used (see Fig-

Instead of replacing the coefficients usually modeled by

ure 2). On vision tasks, performance similar to RBMs can be GMMs, neural networks can also be used to provide additional

achieved by pretraining with "denoising" autoencoders [54] features for the GMM to model [8], [9], [63]. DBN-DNNs have

that are regularized by setting a subset of the inputs to zero or recently been shown to be very effective in such tandem sys-

"contractive" autoencoders [55] that are regularized by penal- tems. On the Aurora2 test set, pretraining decreased WERs by

izing the gradient of the activities of the hidden units with more than one third for speech with signal-to-noise levels of

respect to the inputs. For speech

20 dB or more, though this effect

recognition, improved performance was achieved on both

THE FINE-TUNING OF DNN ACOUSTIC

almost disappeared for very high noise levels [64].

TIMIT and Broadcast News tasks

MODELS IS TYPICALLY STOPPED

Recently, [62] investigated a

by pretraining with a type of

EARLY TO PREVENT OVERFITTING,

less direct way of producing fea-

autoencoder that tries to find sparse codes [56].
ALTERNATIVE FINE-TUNING METHODS FOR DNNs

AND IT IS NOT CLEAR THAT THE MORE SOPHISTICATED METHODS
ARE WORTHWHILE FOR SUCH INCOMPLETE OPTIMIZATION.

ture vectors for the GMM. First, a DNN with six hidden layers of 1,024 units each was trained to achieve good classification accuracy for the 384 HMM states rep-

Very large GMM acoustic models

resented in its softmax output

are trained by making use of the parallelism available in com- layer. This DNN did not have a bottleneck layer and was there-

pute clusters. It is more difficult to use the parallelism of cluster fore able to classify better than a DNN with a bottleneck. Then

systems effectively when training DBN-DNNs. At present, the the 384 logits computed by the DNN as input to its softmax

most effective parallelization method is to parallelize the matrix layer were compressed down to 40 values using a 384-128-40-

operations using a GPU. This gives a speed-up of between one 384 autoencoder. This method of producing feature vectors is

and two orders of magnitude, but the fine-tuning stage remains called AE-BN because the bottleneck is in the autoencoder rath-

a serious bottleneck, and more effective ways of parallelizing er than in the DNN that is trained to classify HMM states.

training are needed. Some recent attempts are described in [52]

Bottleneck feature experiments were conducted on 50-h and

and [57].

430-h of data from the 1996 and 1997 English Broadcast News

Most DBN-DNN acoustic models are fine-tuned by applying Speech collections and English broadcast audio from TDT-4.

stochastic gradient descent with momentum to small mini- The baseline GMM-HMM acoustic model trained on 50 h was

batches of training cases. More sophisticated optimization the same acoustic model described in the section "English

methods that can be used on larger minibatches include nonlin- Broadcast News Speech Recognition Task." The acoustic model

ear conjugate-gradient [17], LBFGS [58], and "Hessian-free" trained on 430-h had 6,000 states and 150,000 Gaussians. Again,

methods adapted to work for DNNs [59]. However, the fine-tun- the standard IBM LVCSR recipe described in the aforementioned

ing of DNN acoustic models is typically stopped early to prevent section was used to create a set of SA DT features and models.

overfitting, and it is not clear that the more sophisticated meth-

All DBN-DNNs used SAT features as input. They were pre-

ods are worthwhile for such incomplete optimization.

trained as DBNs and then discriminatively fine-tuned to predict

target values for 384 HMM states that were obtained by cluster-

OTHER WAYS OF USING DEEP NEURAL

ing the context-dependent states in the baseline GMM-HMM

NETWORKS FOR SPEECH RECOGNITION

system. As in the section "English Broadcast News Speech

The previous section reviewed experiments in which GMMs Recognition Task," the DBN-DNN was trained using the cross

were replaced by DBN-DNN acoustic models to give hybrid entropy criterion, followed by the sequence criterion with the

DNN-HMM systems in which the posterior probabilities over same annealing and stopping rules.

HMM states produced by the DBN-DNN replace the GMM out-

put model. In this section, we describe two other ways of using

DNNs for speech recognition.

Input Units

Code Units

Output Units

USING DBN-DNNs TO PROVIDE INPUT FEATURES FOR GMM-HMM SYSTEMS Here we describe a class of methods where neural networks are used to provide the feature vectors that the GMM in a GMMHMM system is trained to model. The most common approach to extracting these feature vectors is to discriminatively train a randomly initialized neural net with a narrow bottleneck middle layer and to use the activations of the bottleneck hidden units as features. For a summary of such methods, commonly known as the tandem approach, see [60], [61], and [63].

[FIG2] An autoencoder is trained to minimize the discrepancy between the input vector and its reconstruction of the input vector on its output units. If the code units and the output units are both linear and the discrepancy is the squared reconstruction error, an autoencoder finds the same solution as principal components analysis (PCA) (up to a rotation of the components). If the output units and the code units are logistic, an autoencoder is quite similar to an RBM that is trained using CD, but it does not work as well for pretraining DNNs unless it is strongly regularized in an appropriate way. If extra hidden layers are added before and/or after the code layer, an autoencoder can compress data much better than PCA [17].

IEEE SIGNAL PROCESSING MAGAZINE [93] NOVEMBER 2012

After the training of the first DBN-DNN terminated, the final phones, each modeled by a mixture of eight Gaussians. The

set of weights was used for generating the 384 logits at the out- attribute labels were generated by mapping phone labels to

put layer. A second 384-128-40-384 DBN-DNN was then trained attributes, simplifying the overlapping characteristics of the

as an autoencoder to reduce the dimensionality of the output articulatory features. The 22 attributes used in the recent

logits. The GMM-HMM system that used the feature vectors work, as reported in [65], are a subset of the articulatory fea-

produced by the AE-BN was trained using feature and model tures explored in [66] and [67].

space discriminative training. Both pretraining and the use of

DBN-DNNs achieved less than half the error rate of shallow

deeper networks made the AE-BN features work better for rec- neural nets with a single hidden layer. DNN architectures with

ognition. To fairly compare the performance of the system that five to seven hidden layers and up to 2,048 hidden units per

used the AE-BN features with the baseline GMM-HMM system, layer were explored, producing greater than 90% frame-level

the acoustic model of the AE-BN features was trained with the accuracy for all 21 attributes tested in the full DNN system. On

same number of states and Gaussians as the baseline system.

the same data, DBN-DNNs also achieved a very high per frame

Table 4 shows the results of

phone classification accuracy of

the AE-BN and baseline systems on both 50- and 430-h, for different steps in the LVCSR recipe described in the section "English Broadcast News Speech Rec-

THE SUCCESSES ACHIEVED USING PRETRAINING LED TO A RESURGENCE
OF INTEREST IN DNNS FOR ACOUSTIC MODELING.

86.6%. This level of accuracy for detecting subphonetic fundamental speech units may allow a new family of flexible speech recognition and understanding

ognition Task." On 50-h, the

systems that make use of phono-

AE-BN system offers a 1.3% absolute improvement over the logical features in the full detection-based framework dis-

baseline GMM-HMM system, which is the same improvement cussed in [65].

as the DBN-DNN, while on 430-h the AE-BN system provides a

0.5% improvement over the baseline. The 17.5% WER is the SUMMARY AND FUTURE DIRECTIONS

best result to date on the Dev-04f task, using an acoustic model When GMMs were first used for acoustic modeling, they were

trained on 50 h of data. Finally, the complementarity of the trained as generative models using the EM algorithm, and it

AE-BN and baseline methods is explored by performing model was some time before researchers showed that significant gains

combination on both the 50- and 430-h tasks. Table 4 shows could be achieved by a subsequent stage of discriminative train-

that model-combination provides an additional 1.1% absolute ing using an objective function more closely related to the ulti-

improvement over individual systems on the 50-h task, and a mate goal of an ASR system [7], [68]. When neural nets were

0.5% absolute improvement over the individual systems on the first used, they were trained discriminatively. It was only recent-

430-h task, confirming the complementarity of the AE-BN and ly that researchers showed that significant gains could be

baseline systems.

achieved by adding an initial stage of generative pretraining that

completely ignores the ultimate goal of the system. The pre-

USING DNNs TO ESTIMATE ARTICULATORY FEATURES

training is much more helpful in deep neural nets than in shal-

FOR DETECTION-BASED SPEECH RECOGNITION

low ones, especially when limited amounts of labeled training

A recent study [65] demonstrated the effectiveness of DBN- data are available. It reduces overfitting, and it also reduces the

DNNs for detecting subphonetic speech attributes (also known time required for discriminative fine-tuning with backpropaga-

as phonological or articulatory features [66]) in the widely tion, which was one of the main impediments to using DNNs

used The Wall Street Journal speech database (5k-WSJ0). when neural networks were first used in place of GMMs in the

Thirteen MFCCs plus first- and second-temporal derivatives 1990s. The successes achieved using pretraining led to a resur-

were used as the short-time spectral representation of the gence of interest in DNNs for acoustic modeling.

speech signal. The phone labels were derived from the forced Retrospectively, it is now clear that most of the gain comes from

alignments generated using a GMM-HMM system trained with using DNNs to exploit information in neighboring frames and

ML, and that HMM system had 2,818 tied-state, crossword tri- from modeling tied context-dependent states. Pretraining is

helpful in reducing overfitting, and it does reduce the time

taken for fine-tuning, but similar reductions in training time

[TABLE 4] WER IN % ON ENGLISH BROADCAST NEWS.

can be achieved with less effort by careful choice of the scales of

LVCSR STAGE

50 H
GMM-HMM BASELINE AE-BN

430 H
GMM/HMM BASELINE AE-BN

the initial random weights in each layer. The first method to be used for pretraining DNNs was to
learn a stack of RBMs, one per hidden layer of the DNN. An

FSA

24.8

20.6

20.2

17.6

RBM is an undirected generative model that uses binary latent

+fBMMI

20.7

19.0

17.7

16.6

variables, but training it by ML is expensive, so a much faster,

+BMMI

19.6

+MLLR

18.8

MODEL COMBINATION

18.1 17.5 16.4

16.5

15.8

16.0

15.5

15.0

approximate method called CD is used. This method has strong similarities to training an autoencoder network (a nonlinear version of PCA) that converts each datapoint into a code from

IEEE SIGNAL PROCESSING MAGAZINE [94] NOVEMBER 2012

which it is easy to approximately reconstruct the datapoint. he is currently a principal researcher. Prior to MSR, he also

Subsequent research showed that autoencoder networks with worked or taught at Massachusetts Institute of Technology, ATR

one layer of logistic hidden units also work well for pretraining, Interpreting Telecommunications Research Laboratories (Kyoto,

especially if they are regularized by adding noise to the inputs Japan), and Hong Kong University of Science and Technology. In

or by constraining the codes to be insensitive to small changes the general areas of speech recognition, signal processing, and

in the input. RBMs do not require such regularization because machine learning, he has published over 300 refereed papers in

the Bernoulli noise introduced by using stochastic binary hid- leading journals and conferences and three books. He is a Fellow

den units acts as a very strong regularizer [21].

of the Acoustical Society of America, the International Speech

We have described how three major speech research groups Communication Association (ISCA) and the IEEE. He was ISCA's

achieved significant improvements in a variety of state-of-the- Distinguished Lecturer in 2010­2011. He has been granted over

art ASR systems by replacing GMMs with DNNs, and we believe 50 patents and has received awards/honors bestowed by the IEEE,

that there is the potential for

ISCA, the Acoustical Society

considerable further improvement. There is no reason to

CURRENTLY, THE BIGGEST

of America (ASA), Microsoft, and other organizations including

believe that we are currently

DISADVANTAGE OF DNNs

the latest 2011 IEEE Signal

using the optimal types of hid-

COMPARED WITH GMMs IS THAT IT IS

Processing Society (SPS) Mer-

den units or the optimal network architectures, and it is highly likely that both the pretraining and fine-tuning algorithms can

MUCH HARDER TO MAKE GOOD USE OF LARGE CLUSTER MACHINES TO TRAIN THEM ON MASSIVE DATA SETS.

itorious Service Award. He served on the Board of Governors of the IEEE SPS (2008­2010), and as editor-in-chief of IEEE Signal

be modified to reduce the

Processing Magazine (2009­

amount of overfitting and the amount of computation. We 2011). He is currently the editor-in-chief of IEEE Transactions on

therefore expect that the performance gap between acoustic Audio, Speech, and Language Processing (2012­2014). He is the

models that use DNNs and ones that use GMMs will continue to general chair of the International Conference on Acoustics,

increase for some time.

Speech, and Signal Processing (ICASSP) 2013.

Currently, the biggest disadvantage of DNNs compared with

Dong Yu (dongyu@ieee.org) received a Ph.D. degree in com-

GMMs is that it is much harder to make good use of large clus- puter science from the University of Idaho, an M.S. degree in

ter machines to train them on massive data sets. This is offset computer science from Indiana University at Bloomington, an

by the fact that DNNs make more efficient use of data so they do M.S. degree in electrical engineering from the Chinese Academy

not require as much data to achieve the same performance, but of Sciences, and a B.S. degree (with honors) in electrical engi-

better ways of parallelizing the fine-tuning of DNNs is still a neering from Zhejiang University (China). He joined Microsoft

major issue.

Corporation in 1998 and MSR in 2002, where he is a researcher.

His current research interests include speech processing, robust

AUTHORS

speech recognition, discriminative training, spoken dialog sys-

Geoffrey Hinton (geoffrey.hinton@gmail.com) received his tem, voice search technology, machine learning, and pattern

Ph.D. degree from the University of Edinburgh in 1978. He recognition. He has published more than 90 papers in these

spent five years as a faculty member at Carnegie Mellon areas and is the inventor/coinventor of more than 40 granted/

University, Pittsburgh, Pennsylvania, and he is currently a dis- pending patents. He is currently an associate editor of IEEE

tinguished professor at the University of Toronto. He is a fellow Transactions on Audio, Speech, and Language Processing

of the Royal Society and an honorary foreign member of the (2011­present) and has been an associate editor of IEEE Signal

American Academy of Arts and Sciences. His awards include the Processing Magazine (2008­2011) and was the lead guest editor

David E. Rumelhart Prize, the International Joint Conference of the Special Issue on Deep Learning for Speech and Language

on Artificial Intelligence Research Excellence Award, and the Processing (2010­2011), IEEE Transactions on Audio, Speech,

Gerhard Herzberg Canada Gold Medal for Science and and Language Processing.

Engineering. He was one of the researchers who introduced the

George E. Dahl (george.dahl@gmail.com) received a

back-propagation algorithm. His other contributions include B.A. degree in computer science with highest honors from

Boltzmann machines, distributed representations, time-delay Swarthmore College and an M.Sc. degree from the University of

neural nets, mixtures of experts, variational learning, CD learn- Toronto, where he is currently completing a Ph.D. degree with a

ing, and DBNs.

research focus in statistical machine learning. His current main

Li Deng (deng@microsoft.com) received his Ph.D. degree research interest is in training models that learn many levels of

from the University of Wisconsin­Madison. In 1989, he joined the rich, distributed representations from large quantities of per-

Department of Electrical and Computer Engineering at the ceptual and linguistic data.

University of Waterloo, Ontario, Canada, as an assistant professor,

Abdel-rahman Mohamed (asamir@cs.toronto.edu) received

where he became a tenured full professor in 1996. In 1999, he his B.Sc. and M.Sc. degrees from the Department of Electronics

joined MSR, Redmond, Washington, as a senior researcher, where and Communication Engineering, Cairo University in 2004 and

IEEE SIGNAL PROCESSING MAGAZINE [95] NOVEMBER 2012

2007, respectively. In 2004, he worked in the speech research group at RDI Company, Egypt. He then joined the ESAT-PSI speech group at the Katholieke Universiteit Leuven, Belgium. In September 2008, he started his Ph.D. degree at the University of Toronto. His research focus is in developing machine learning techniques to advance human language technologies.
Navdeep Jaitly (ndjaitly@yahoo.com) received his B.A. degree from Hanover College and an M.Math degree from the University of Waterloo in 2000. After receiving his master's degree, he developed algorithms and statistical methods for analysis of protoemics data at Caprion Pharmaceuticals in Montreal and at Pacific Northwest National Labs in Washington. Since 2008, he has been pursuing a Ph.D. degree at the University of Toronto. His current interests lie in machine learning, speech recognition, computational biology, and statistical methods.
Andrew Senior (andrewsenior@google.com) received his Ph.D. degree from the University of Cambridge and is a research scientist at Google. Before joining Google, he worked at IBM Research in the areas of handwriting, audio-visual speech, face, and fingerprint recognition as well as video privacy protection and visual tracking. He edited Privacy Protection in Video Surveillance, coauthored Springer's Guide to Biometrics and over 60 scientific papers, holds 26 patents, and is an associate editor of the journal Pattern Recognition. His research interests range across speech and pattern recognition, computer vision, and visual art.
Vincent Vanhoucke (vanhoucke@google.com) received his Ph.D. degree from Stanford University in 2004 for research in acoustic modeling and is a graduate from the Ecole Centrale Paris. From 1999 to 2005, he was a research scientist with the speech R&D team at Nuance, in Menlo Park, California. He is currently a research scientist at Google Research, Mountain View, California, where he manages the speech quality research team. Previously, he was with Like.com (now part of Google), where he worked on object, face, and text recognition technologies.
Patrick Nguyen (drpng@google.com) received his doctorate degree from the Swiss Federal Institute for Technology (EPFL) in 2002. In 1998, he founded a company developing a platform realtime foreign exchange trading. He was with the Panasonic Speech Technology Laboratory from 2000 to 2004, in Santa Barbara, California, and MSR in Redmond, Washington, from 2004 to 2010. He is currently a research scientist at Google Research, Mountain View, California. His area of expertise revolves around statistical processing of human language, and in particular, speech recognition. He is mostly known for segmental conditional random fields and eigenvoices. He was on the organizing committee of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding and he co-led the 2010 Johns Hopkins University Workshop on Speech Recognition. He currently serves on the Speech and Language Technical Committee of the IEEE SPS.
Tara Sainath (tsainath@us.ibm.com) received her Ph.D. degree in electrical engineering and computer science from Massachusetts Institute of Technology in 2009. The main focus of her Ph.D. work was in acoustic modeling for noise robust

speech recognition. She joined the Speech and Language Algorithms group at IBM T.J. Watson Research Center upon completion of her Ph.D. degree. She organized a special session on sparse representations at INTERSPEECH 2010 in Japan. In addition, she has been a staff reporter of IEEE Speech and Language Processing Technical Committee Newsletter. She currently holds 15 U.S. patents. Her research interests mainly focus in acoustic modeling, including sparse representations, DBN works, adaptation methods, and noise robust speech recognition.
Brian Kingsbury (bedk@us.ibm.com) received the B.S. degree (high honors) in electrical engineering from Michigan State University, East Lansing, in 1989 and the Ph.D. degree in computer science from the University of California, Berkeley, in 1998. Since 1999, he has been a research staff member in the Department of Human Language Technologies, IBM T.J. Watson Research Center, Yorktown Heights, New York. His research interests include large-vocabulary speech transcription, audio indexing and analytics, and information retrieval from speech. From 2009 to 2011, he served on the IEEE SPS's Speech and Language Technical Committee, and from 2010 to 2012 he was an ICASSP area chair. He is currently an associate editor of IEEE Transactions on Audio, Speech, and Language Processing.
REFERENCES
[1] J. Baker, L. Deng, J. Glass, S. Khudanpur, Chin Hui Lee, N. Morgan, and D. O'Shaughnessy, "Developments and directions in speech recognition and understanding, part 1," IEEE Signal Processing Mag., vol. 26, no. 3, pp. 75­80, May 2009.
[2] S. Furui, Digital Speech Processing, Synthesis, and Recognition. New York: Marcel Dekker, 2000.
[3] B. H. Juang, S. Levinson, and M. Sondhi, "Maximum likelihood estimation for multivariate mixture observations of Markov chains," IEEE Trans. Inform. Theory, vol. 32, no. 2, pp. 307­309, 1986.
[4] H. Hermansky, "Perceptual linear predictive (PLP) analysis of speech," J. Acoust. Soc. Amer., vol. 87, no. 4, pp. 1738­1752, 1990.
[5] S. Furui, "Cepstral analysis technique for automatic speaker verification," IEEE Trans. Acoust., Speech, Signal, Processing, vol. 29, no. 2, pp. 254­272, 1981.
[6] S. Young, "Large vocabulary continuous speech recognition: A review," IEEE Signal Processing Mag., vol. 13, no. 5, pp. 45­57, 1996.
[7] L. Bahl, P. Brown, P. de Souza, and R. Mercer, "Maximum mutual information estimation of hidden Markov model parameters for speech recognition," in Proc. ICASSP, 1986, pp. 49­52.
[8] H. Hermansky, D. P. W. Ellis, and S. Sharma, "Tandem connectionist feature extraction for conventional HMM systems," in Proc. ICASSP. Los Alamitos, CA: IEEE Computer Society, 2000, vol. 3, pp. 1635­1638.
[9] H. Bourlard and N. Morgan, Connectionist Speech Recognition: A Hybrid Approach, Norwell, MA: Kluwer, 1993.
[10] L. Deng, "Computational models for speech production," in Computational Models of Speech Pattern Processing, K. M. Ponting, Ed. New York: SpringerVerlag, 1999, pp. 199­213.
[11] L. Deng, "Switching dynamic system models for speech articulation and acoustics," in Mathematical Foundations of Speech and Language Processing, M. Johnsonm S. P. Khudanpur, M. Ostendorf, and R. Rosenfeld. New York: Springer-Verlag, 2003, pp. 115­134.
[12] A. Mohamed, G. Dahl, and G. Hinton, "Deep belief networks for phone recognition," in Proc. NIPS Workshop Deep Learning for Speech Recognition and Related Applications, 2009.
[13] A. Mohamed, G. Dahl, and G. Hinton, "Acoustic modeling using deep belief networks," IEEE Trans. Audio Speech Lang. Processing, vol. 20, no. 1, pp. 14­22, Jan. 2012.
[14] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning representations by back-propagating errors," Nature, vol. 323, no. 6088, pp. 533­536, 1986.
[15] X. Glorot and Y. Bengio, "Understanding the difficulty of training deep feedforward neural networks," in Proc. AISTATS, 2010, pp. 249­256.

IEEE SIGNAL PROCESSING MAGAZINE [96] NOVEMBER 2012

[16] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber, "Deep, big, simple neural nets for handwritten digit recognition," Neural Comput., vol. 22, no. 12, pp. 3207­3220, 2010.
[17] G. E. Hinton and R. Salakhutdinov, "Reducing the dimensionality of data with neural networks," Science, vol. 313, no. 5786, pp. 504­507, 2006.
[18] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio, "An empirical evaluation of deep architectures on problems with many factors of variation," in Proc. 24th Int. Conf. Machine Learning, 2007, pp. 473­480.
[19] J. Pearl, Probabilistic Inference in Intelligent Systems: Networks of Plausible Inference. San Mateo, CA: Morgan Kaufmann, 1988.
[20] G. E. Hinton, "Training products of experts by minimizing contrastive divergence," Neural Comput., vol. 14, pp. 1771­1800, 2002.
[21] G. E. Hinton, "A practical guide to training restricted Boltzmann machines," Tech. Rep. UTML TR 2010-003, Dept. Comput. Sci., Univ. Toronto, 2010.
[22] G. E. Hinton, S. Osindero, and Y. Teh, "A fast learning algorithm for deep belief nets," Neural Comput., vol. 18, no. 7, pp. 1527­1554, 2006.
[23] T. N. Sainath, B. Ramabhadran, and M. Picheny, "An exploration of large vocabulary tools for small vocabulary phonetic recognition," in Proc. IEEE Automatic Speech Recognition and Understanding Workshop, 2009, pp. 359­364.
[24] A. Mohamed, T. N. Sainath, G. E. Dahl, B. Ramabhadran, G. E. Hinton, and M. Picheny, "Deep belief networks using discriminative features for phone recognition," in Proc. ICASSP, 2011, pp. 5060­5063.
[25] A. Mohamed, G. Hinton, and G. Penn, "Understanding how deep belief networks perform acoustic modelling," in Proc. ICASSP, 2012, pp. 4273­4276.
[26] Y. Hifny and S. Renals, "Speech recognition using augmented conditional random fields," IEEE Trans. Audio Speech Lang. Processing, vol. 17, no. 2, pp. 354­365, 2009.
[27] A. Robinson, "An application to recurrent nets to phone probability estimation," IEEE Trans. Neural Networks, vol. 5, no. 2, pp. 298­305, 1994.
[28] J. Ming and F. J. Smith, "Improved phone recognition using Bayesian triphone models," in Proc. ICASSP, 1998, pp. 409­412.
[29] L. Deng and D. Yu, "Use of differential cepstra as acoustic features in hidden trajectory modelling for phonetic recognition," in Proc. ICASSP, 2007, pp. 445­448.
[30] A. Halberstadt and J. Glass, "Heterogeneous measurements and multiple classifiers for speech recognition," in Proc. ICSLP, 1998.
[31] A. Mohamed, D. Yu, and L. Deng, "Investigation of full-sequence training of deep belief networks for speech recognition," in Proc. Interspeech, 2010, pp. 2846­2849.
[32] T. N. Sainath, B. Ramabhadran, M. Picheny, D. Nahamoo, and D. Kanevsky, "Exemplar-based sparse representation features: From TIMIT to LVCSR," IEEE Trans. Audio Speech Lang. Processing, vol. 19, no. 8, pp. 2598­2613, Nov. 2011.
[33] G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton, "Phone recognition with the mean-covariance restricted Boltzmann machine," in Advances in Neural Information Processing Systems 23, J. Lafferty, C. K. I. Williams, J. ShaweTaylor, R.S. Zemel, and A. Culotta, Eds. Cambridge, MA: MIT Press, 2010, pp. 469­477.
[34] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn, "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition," in Proc. ICASSP, 2012, pp. 4277­4280.
[35] X. He, L. Deng, and W. Chou, "Discriminative learning in sequential pattern recognition--A unifying review for optimization-oriented speech recognition," IEEE Signal Processing Mag., vol. 25, no. 5, pp. 14­36, 2008.
[36] Y. Bengio, R. De Mori, G. Flammia, and F. Kompe, "Global optimization of a neural network--Hidden Markov model hybrid," in Proc. EuroSpeech, 1991.
[37] B. Kingsbury, "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling," in Proc. ICASSP, 2009, pp. 3761­3764.
[38] R. Prabhavalkar and E. Fosler-Lussier, "Backpropagation training for multilayer conditional random field based phone recognition," in Proc. ICASSP, 2010, pp. 5534­5537.
[39] H. Lee, P. Pham, Y. Largman, and A. Ng, "Unsupervised feature learning for audio classification using convolutional deep belief networks," in Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, Eds. Cambridge, MA: MIT Press, 2009, pp. 1096­1104.
[40] L. Deng, D. Yu, and A. Acero, "Structured speech modeling," IEEE Trans. Audio Speech Lang. Processing, vol. 14, no. 5, pp. 1492­1504, 2006.
[41] H. Zen, M. Gales, Y. Nankaku, and K. Tokuda, "Product of experts for statistical parametric speech synthesis," IEEE Trans. Audio Speech and Lang. Processing, vol. 20, no. 3, pp. 794­805, Mar. 2012.
[42] G. Dahl, D. Yu, L. Deng, and A. Acero, "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition," IEEE Trans. Audio Speech Lang. Processing, vol. 20, no. 1, pp. 30­42, Jan. 2012.
[43] F. Seide, G. Li, and D. Yu, "Conversational speech transcription using context-dependent deep neural networks," in Proc. Interspeech, 2011, pp. 4 37­ 4 40.

[44] D. Yu, L. Deng, and G. Dahl, "Roles of pretraining and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition," in Proc. NIPS Workshop Deep Learning and Unsupervised Feature Learning, 2010.
[45] F. Seide, G. Li, X. Chen, and D. Yu, "Feature engineering in context-dependent deep neural networks for conversational speech transcription," in Proc. IEEE ASRU, 2011, pp. 24­29.
[46] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah, "Boosted MMI for model and feature-space discriminative training," in Proc. ICASSP, 2008, pp. 4057­4060.
[47] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke, "An application of pretrained deep neural networks to large vocabulary speech recognition," submitted for publication.
[48] G. Zweig, P. Nguyen, D. V. Compernolle, K. Demuynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha, H. Hermansky, D. Karakos, A. Jansen, S. Thomas, G. S. V. S. Sivaram, S. Bowman, and J. Kao, "Speech recognition with segmental conditional random fields: A summary of the JHU CLSP 2010 summer workshop," in Proc. ICASSP, 2011, pp. 5044­5047.
[49] V. Vanhoucke, A. Senior, and M. Z. Mao, "Improving the speed of neural networks on CPUs," in Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011 [Online]. Available: http://research.google.com/pubs/ archive/37631.pdf
[50] T. N. Sainath, B. Kingsbury, and B. Ramabhadran, "Improvements in using deep belief networks for large vocabulary continuous speech recognition," Speech and Language Algorithm Group, IBM, Yorktown Heights, NY, Tech. Rep. UTML TR 2010-003, Feb. 2011.
[51] L. Deng and D. Yu, "Deep convex network: A scalable architecture for speech pattern classification," in Proc. Interspeech, 2011, pp. 2285­2288.
[52] L. Deng, D. Yu, and J. Platt, "Scalable stacking and learning for building deep architectures," in Proc. ICASSP, 2012, pp. 2133­2136.
[53] D. Yu, L. Deng, G. Li, and Seide F, "Discriminative pretraining of deep neural networks," U.S. Patent Filing, Nov. 2011.
[54] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion," J. Mach. Learn. Res., vol. 11, no. 11, pp. 3371­3408, 2010.
[55] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, "Contractive autoencoders: Explicit invariance during feature extraction," in Proc. 28th Int. Conf. Machine Learning, 2011, pp. 833­840.
[56] C. Plahl, T. N. Sainath, B. Ramabhadran, and D. Nahamoo, "Improved pretraining of deep belief networks using sparse encoding symmetric machines," in Proc. ICASSP, 2012, pp. 4165­4168.
[57] B. Hutchinson, L. Deng, and D. Yu, "A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition," in Proc. ICASSP, 2012, pp. 4805­4808.
[58] Q. V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Y. Ng, "On optimization methods for deep learning," in Proc. 28th Int. Conf. Machine Learning, 2011, pp. 265­272.
[59] J. Martens, "Deep learning via Hessian-free optimization," in Proc. 27th Int. Conf. Machine learning, 2010, pp. 735­742.
[60] N. Morgan, "Deep and wide: Multiple layers in automatic speech recognition," IEEE Trans. Audio Speech Lang. Processing, vol. 20, no. 1, Jan. 2012, pp. 7­13.
[61] G. Sivaram and H. Hermansky, "Sparse multilayer perceptron for phoneme recognition," IEEE Trans. Audio Speech Lang. Processing, vol. 20, no. 1, Jan. 2012, pp. 23­29.
[62] T. N. Sainath, B. Kingsbury, and B. Ramabhadran, "Auto-encoder bottleneck features using deep belief networks," in Proc. ICASSP, 2012, pp. 4153­4156.
[63] N. Morgan, Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki, M. Ostendorf, P. Jain, H. Hermansky, D. Ellis, G. Doddington, B. Chen, O. Cretin, H. Bourlard, and M. Athineos, "Pushing the envelope aside," IEEE Signal Processing Mag., vol. 22, no. 5, pp. 81­88, Sept. 2005.
[64] O. Vinyals and S. V. Ravuri, "Comparing multilayer perceptron to deep belief network tandem features for robust ASR," in Proc. ICASSP, 2011, pp. 4596­4599.
[65] D. Yu, S. Siniscalchi, L. Deng, and C. Lee, "Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition," in Proc. ICASSP, 2012, pp. 4169­4172.
[66] L. Deng and D. Sun, "A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features," J. Acoust. Soc. Amer., vol. 85, no. 5, pp. 2702­2719, 1994.
[67] J. Sun and L. Deng, "An overlapping-feature based phonological model incorporating linguistic constraints: Applications to speech recognition," J. Acoustic. Soc. Amer., vol. 111, no. 2, pp. 1086­1101, 2002.
[68] P. C. Woodland and D. Povey, "Large scale discriminative training of hidden Markov models for speech recognition," Comput Speech Lang., vol. 16, no. 1, pp. 25­47, 2002.
[69] F. Grezl, M. Karaat, S. Kontar, and J. Cernocky. "Probabilistic and bottle-neck features for LVCSR of meetings," in Proc. ICASSP, 2007.
[SP]

IEEE SIGNAL PROCESSING MAGAZINE [97] NOVEMBER 2012

